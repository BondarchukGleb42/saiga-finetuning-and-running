# saiga_finetune_and_running
RuLLM Saiga finetuning, inference with llama cpp library and running in Docker with FastAPI
